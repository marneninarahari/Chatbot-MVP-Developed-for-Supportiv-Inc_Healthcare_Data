{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1Wiz3L3eSurNTMfS0MLCW7a_Pfhk3AjVD","timestamp":1756056532441}],"gpuType":"T4","authorship_tag":"ABX9TyN0rEs9m616bbQVhnP/Me1t"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","cells":[{"cell_type":"markdown","source":["# Chatbot with Generative Pretrained Transformer(GPT) Model\n","#### The objective of this project is to develop a Chatbot using a pretrained text generation model (GPT-model) and go over the process of fine-tuning it on the given dataset. This fine-tuning step is key in producing highquality models. Fine-tuning allows us to adapt a model to a specific dataset or domain.\n","\n","#### The supervised fine-tuning method which is a most common method for fine-tuning text generation models has been implimented. The transformative potential of finetuning pretrained text generation models is to make them more effective tools for the application.\n","\n"],"metadata":{"id":"ScwwKg6kIbfB"}},{"cell_type":"markdown","source":["# Install the Required Libraries"],"metadata":{"id":"-wl-4kASR5Sp"}},{"cell_type":"code","source":["%%capture\n","!pip install -q accelerate==0.31.0 peft==0.11.1 bitsandbytes==0.43.1 transformers==4.41.2 trl==0.9.4 sentencepiece==0.2.0 triton==3.1.0"],"metadata":{"id":"qLWIyaJ_OwJn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import os\n","import torch\n","from datasets import load_dataset\n","from transformers import (\n","    AutoTokenizer,\n","    AutoModelForCausalLM,\n","    BitsAndBytesConfig\n",")\n","from peft import LoraConfig\n","from trl import SFTTrainer, SFTConfig, DataCollatorForCompletionOnlyLM"],"metadata":{"id":"aP20hBXMDrxo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Supervised Fine-Tuning (SFT)\n","#### With supervised fine-tuning (SFT), we can adapt the base model to follow instructions. During this fine-tuning process, the parameters of the base model are updated to be more in line with our target task, like following instructions. Like a pretrained model, it is trained using next-token prediction but instead of only predicting the next token, it does so based on a user input."],"metadata":{"id":"_cEwwbtjQNZn"}},{"cell_type":"markdown","source":["# Data Importing and Preprocessing\n","\n","#### Import the data from CSV file and prepare the data for traing the LLM.  \n"],"metadata":{"id":"DW6RttHirxcS"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OJ9gNO4iab9_","executionInfo":{"status":"ok","timestamp":1756072781237,"user_tz":420,"elapsed":35399,"user":{"displayName":"Narahari Marneni","userId":"11245013731051180426"}},"outputId":"ecad5ccb-cfef-450e-e4c8-2336f9dfbac7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["csv_path = \"/content/drive/MyDrive/Chatbot_assignment_files/mle_screening_dataset.csv\"\n","df = pd.read_csv(csv_path)\n","\n","print(df.head())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1gx9mJ4FanRQ","executionInfo":{"status":"ok","timestamp":1756072790492,"user_tz":420,"elapsed":2250,"user":{"displayName":"Narahari Marneni","userId":"11245013731051180426"}},"outputId":"9b4110f7-aab2-4460-e47b-265ddede029e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["                         question  \\\n","0        What is (are) Glaucoma ?   \n","1        What is (are) Glaucoma ?   \n","2        What is (are) Glaucoma ?   \n","3  Who is at risk for Glaucoma? ?   \n","4       How to prevent Glaucoma ?   \n","\n","                                              answer  \n","0  Glaucoma is a group of diseases that can damag...  \n","1  The optic nerve is a bundle of more than 1 mil...  \n","2  Open-angle glaucoma is the most common form of...  \n","3  Anyone can develop glaucoma. Some people are a...  \n","4  At this time, we do not know how to prevent gl...  \n"]}]},{"cell_type":"code","source":["# User config\n","BASE_MODEL = os.environ.get(\"BASE_MODEL\", \"Qwen/Qwen2-0.5B-Instruct\")\n","CSV_PATH   = os.environ.get(\"CSV_PATH\",   \"/content/drive/MyDrive/Chatbot_assignment_files/mle_screening_dataset.csv\")\n","OUTPUT_DIR = os.environ.get(\"OUTPUT_DIR\", \"qwen2-sft-chatbot\")\n","USE_4BIT   = os.environ.get(\"USE_4BIT\",   \"true\").lower() == \"true\""],"metadata":{"id":"iKi_KVfqtsDI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load CSV\n","# Expects exactly two columns: \"question\", \"answer\"\n","dataset = load_dataset(\"csv\", data_files={\"train\": CSV_PATH})[\"train\"]\n","\n","# Build a single 'text' field per row in the classic promptâ†’response format.\n","# We'll compute loss **only on the answer tokens** using a special collator.\n","RESPONSE_PREFIX = \"Assistant:\"\n","\n","def row_to_text(example):\n","    q = str(example[\"question\"]).strip()\n","    a = str(example[\"answer\"]).strip()\n","    # The \"Assistant:\" prefix is important: our collator will mask loss before it.\n","    # Keep the prompt short, consistent, and aligned with how you'll chat at inference.\n","    return {\"text\": f\"User: {q}\\n{RESPONSE_PREFIX} {a}\"}\n","\n","dataset = dataset.map(row_to_text, remove_columns=dataset.column_names)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":81,"referenced_widgets":["c27271bccc304f0598c45792793fbc3c","c8ecb451ce66420f9f95ab44944c6be6","285c1c555c99417fa706f92e3c5e508e","b93e5ff47e644401825fb9ab33948603","272d86cd6a76431ea36ba05273d018fd","9ab0faa5983348f4a53a1eaf1028d0ce","851560d2ce514406a9e42a26c85e05c3","df8eddfebe8f4ca9b73822d46435fad5","683ed1431274496a8b280a2ffbe8a8ce","3b8cdb023d194ac1a7172d6cb1003f8a","9dc205f48f384c0caebee40b4fe494a8","4429a33c9b434b4d9fe1627b49ca8f49","69c16ac10a7f48ad98de9dff7b903b13","84c99fe916fc41ba9c98c34b181c9a93","cd49c6ae7ab64310a8db4d8da2682238","2aa11f725a10432085f1156eb1a6e93f","e699242cb94e4271914a3da53cd64756","c48bbc1f86e54cd68453afda256518a9","e8dfc42db1ba4f268883eef34c96af6b","ea9b6c30a8ea4056b1ecb8ec6b2214f4","614967463c1f4a08b05d8ec28291bd67","8673ebba546d4e5c8ba2ee6b335824d3"]},"id":"Sn_BhDIpvWcg","executionInfo":{"status":"ok","timestamp":1756072808215,"user_tz":420,"elapsed":1595,"user":{"displayName":"Narahari Marneni","userId":"11245013731051180426"}},"outputId":"d33b54eb-c008-4d88-b423-20fae5e074f4"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Generating train split: 0 examples [00:00, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c27271bccc304f0598c45792793fbc3c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/16406 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4429a33c9b434b4d9fe1627b49ca8f49"}},"metadata":{}}]},{"cell_type":"code","source":["# Example of formatted prompt\n","print(dataset[\"text\"][258])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7DKPZ-q0xIb6","executionInfo":{"status":"ok","timestamp":1756072810875,"user_tz":420,"elapsed":14,"user":{"displayName":"Narahari Marneni","userId":"11245013731051180426"}},"outputId":"9991c2d2-5f31-4a39-a781-cf47dc74dad3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["User: How to diagnose Alzheimer's Disease ?\n","Assistant: The time from diagnosis of Alzheimers disease to death varies. It can be as little as 3 or 4 years if the person is over 80 years old when diagnosed or as long as 10 years or more if the person is younger.\n"]}]},{"cell_type":"code","source":["# Tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n","# Ensure a valid pad token for batching\n","if tokenizer.pad_token_id is None:\n","    tokenizer.pad_token_id = tokenizer.eos_token_id\n","\n","\n","# Collator (mask prompt loss)\n","# Only compute loss on tokens **after** \"Assistant:\" so the model learns the answer, not to copy the question.\n","collator = DataCollatorForCompletionOnlyLM(\n","    response_template=f\"{RESPONSE_PREFIX}\",\n","    tokenizer=tokenizer\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":292,"referenced_widgets":["75e495593fe0480d92a6a3dfbaed73a5","5485246c1f454f90988e56774468cd14","2367407cf6884ffb8ff5388ff3d6166f","b7f0a5c8c74f43cb95905f3e858c69e2","4db35c59410f4c1eae955ddce4b0ae6a","fd7f1c209233496787e53c1edf0cc572","55bc015cfe8d4f379ced69cc7937ed82","e5e569e14e5e4f259fb73645a27a1f63","ef3d000168524dafa3c8ba9096b6d91d","2f9b2cbed66447388ca88ee550c63333","dbd050ee3b29477c813bdd2a6c34c1f7","8b0a977247304fa19baa5976eabc96d5","71dc67272c914551aef05380c753cf44","231574e5f54f4fa4ac4c025a08d4e824","0341f968f55d49b081615070e488af39","12fa28ed23a44388bd1b7cdb402dbacd","e91ac32be7cc400e910a039712d21f65","ee31bcef770f4317980d8c00fee47068","996a1baa5c374916bca3a7f389760584","328c948caff24db0b2148d721ae31e4d","0af3743bef164cd0a7d3eea423cc9c8d","08f6a77f347d4892a05c33ae42cafb96","e6f43b5a548d46f0b1a351a00cbc8d6d","8447381075174d938454efaee03f53a5","8c7fc1178c894f2e83518b19d21807d2","ddb5fb5014bf422d830cddce116d6a53","5b795f86af1741b482849096df8fe188","126eb6d931194598963d60eed2b0e762","dbe4897c655447f086dd5e0a84bdc0a0","c8c5262df4004198aadb5f6a77f59c7d","c45ed26eb4dd43c196c2a71de9f762d0","7ffcede0c0234f17993cc2753a5cd2b7","246d5b3a18754857b495c114627b3a07","7fb627af5b644419bb7ba260347a16e6","b03b35baf5194c91b73376cb91543aaa","aa04ededb46344469110fae29981f514","76d10612f77e41879a8292a4e63b1b86","b83ce13a50ca40fca91e9eb77a0ce9e3","9ae9090dbf6942d68a42df7809a2cff2","e112a2a24f0349598aec9e354138da45","ca7f8b7ad2514ac7b2057598bd7594ab","c527e324cd5c46159623502554046387","e0e92956d4ec4c4ab22919339f440edb","22c9ee3b46c247fcbef7a5fe6b7c57d3"]},"id":"NKwHpM37yDNS","executionInfo":{"status":"ok","timestamp":1756072818603,"user_tz":420,"elapsed":1695,"user":{"displayName":"Narahari Marneni","userId":"11245013731051180426"}},"outputId":"49903f05-b4b5-4c99-cf90-cd7d5780056a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75e495593fe0480d92a6a3dfbaed73a5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.json: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b0a977247304fa19baa5976eabc96d5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["merges.txt: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6f43b5a548d46f0b1a351a00cbc8d6d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7fb627af5b644419bb7ba260347a16e6"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}]},{"cell_type":"markdown","source":["## Models - Quantization\n","#### We have our data, now we can start loading in our model. This is where we apply the Q in QLoRA, namely quantization. Here I have used the \"bitsandbytes\" package to compress the pretrained model to a 4-bit representation. In BitsAndBytesConfig, you can define the quantization scheme. I followed the steps used in the original QLoRA paper and load the model in 4-bit (load_in_4bit) with a normalized float representation (bnb_4bit_quant_type) and double quantization (bnb_4bit_use_double_quant)."],"metadata":{"id":"utcrd7QIRnZf"}},{"cell_type":"code","source":["# Base model\n","model_kwargs = {}\n","if USE_4BIT and torch.cuda.is_available():\n","    model_kwargs[\"quantization_config\"] = BitsAndBytesConfig(\n","        load_in_4bit=True,\n","        bnb_4bit_quant_type=\"nf4\",\n","        bnb_4bit_compute_dtype=torch.bfloat16,\n","        bnb_4bit_use_double_quant=True,\n","    )\n","    model_kwargs[\"device_map\"] = \"auto\"\n","\n","model = AutoModelForCausalLM.from_pretrained(BASE_MODEL, **model_kwargs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":113,"referenced_widgets":["44a0d32c13ea4758a0fd5954018e659d","bea6ff39b7fa4307ab66e796d1084aeb","33c20a3ab78143a6bfb05c20808a57e7","c9ed1c69c294454da8e1cc304bf75520","10de2033287f4b018adac66761ed4824","ff8bad75d0cf4946b98e1e0d6b63dc75","d4286acdf2b04630b121b6e65f50a061","7b6e5350711542eab51f6e4b651f9f43","dc54246cd41b4fe28d2171ab787a8c7c","0fc93b8fef90432c8c60301eb7037ecd","afdbd23c44294db3b9cb5ab0d94391f7","61f688004e5f4883be5c1c01ff0f5106","d2e0f55bd8ee4ca6b564bf6ce6f24b23","7316e7ba90fd40b18dda06ff67932bd3","d2823c743ef94a628d364cee51f4cdc4","6b8e3822cf394a98a1a6d3d0a8a38254","514eda4ce7a7409f859e66fa1151a71a","35691466a9fc4b6ba2c4f416d07dd72b","02e946fdab9d4e959e155b4c536bb269","fe6e20605f8b45eb851366842e55ca75","7650823de16447d88c191b0b33d055c2","35b52b84bcbf4a939f97bb3959495248","0ccf83116861468d9da051bddfd41606","c460e00140244528922b6ecd018cb048","d40bcbead20f461ab3426eadae9bdd03","99bdb4967c6542aaaa3f3a9648b0ee7f","6c588df992cb465fbe74a7c4436b7fca","3f83a25f4ef348b4aa2bde511b844a89","7993bfcc929f466984743756436601a6","e12e1612eb6d4cb597dfe5502d3b0027","a951eeef1a07422590512c468f6044ec","209eadc66da54a8e8f7ac9d14fef97fd","b3c3428bd1eb4916a72855bd799dd89b"]},"id":"vbQv1YVZzlhJ","executionInfo":{"status":"ok","timestamp":1756072868976,"user_tz":420,"elapsed":20298,"user":{"displayName":"Narahari Marneni","userId":"11245013731051180426"}},"outputId":"84980533-221d-44ee-882a-060808216e2f"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/659 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44a0d32c13ea4758a0fd5954018e659d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/988M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61f688004e5f4883be5c1c01ff0f5106"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ccf83116861468d9da051bddfd41606"}},"metadata":{}}]},{"cell_type":"markdown","source":["# Configuration\n","## LoRA Configuration\n","#### Next, I have defined defined LoRA configuration using the \"peft\" library, which represents hyperparameters of the fine-tuning process.\n","\n","### Parameters\n","#### r: This is the rank of the compressed matrices. Increasing this value will also increase the sizes of compressed matrices leading to less compression and thereby improved representative power. Values typically range between 4 and 64.\n","\n","#### lora_alpha: Controls the amount of change that is added to the original weights. In essence, it balances the knowledge of the original model with that of the new task. A rule of thumb is to choose a value twice the size of r.\n","\n","#### target_modules: Controls which layers to target. The LoRA procedure can choose to ignore specific layers, like specific projection layers. This can speed up training but reduce performance and vice versa.\n","\n","#### We can experiment by Playing with the parameter values to get an intuitive understanding of values that work and those that do not for our use case."],"metadata":{"id":"w1_KNmnKTLOu"}},{"cell_type":"code","source":["from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n","\n","# Prepare LoRA Configuration\n","peft_config = LoraConfig(\n","    r=16,\n","    lora_alpha=32,\n","    lora_dropout=0.05,\n","    bias=\"none\",\n","    task_type=\"CAUSAL_LM\",\n","    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",")\n","\n","# prepare model for training\n","model = prepare_model_for_kbit_training(model)\n","model = get_peft_model(model, peft_config)"],"metadata":{"id":"uDgDEbssz8G6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Training Configuration\n","#### Now, we need to configure the training parameters.\n","\n","### There are several parameters worth mentioning:\n","#### num_train_epochs: The total number of training rounds. Higher values tend to degrade performance so we generally like to keep this low.\n","\n","#### learning_rate: Determines the step size at each iteration of weight updates. It is know that higher learning rates work better for larger models (>33B parameters).\n","\n","#### lr_scheduler_type: A cosine-based scheduler to adjust the learning rate dynamically. It will linearly increase the learning rate, starting from zero, until it reaches the set value. After that, the learning rate is decayed following the values of a cosine function.\n","\n","#### optim: The paged optimizers used in the original QLoRA paper."],"metadata":{"id":"Ubnv53b8VkUg"}},{"cell_type":"code","source":["from transformers import TrainingArguments\n","\n","# Training arguments\n","training_arguments = TrainingArguments(\n","    output_dir=OUTPUT_DIR,\n","    per_device_train_batch_size=2,\n","    gradient_accumulation_steps=4,\n","    optim=\"paged_adamw_32bit\",\n","    learning_rate=2e-3,\n","    lr_scheduler_type=\"cosine\",\n","    num_train_epochs=1,\n","    logging_steps=10,\n","    fp16=True,\n","    gradient_checkpointing=True\n",")"],"metadata":{"id":"X4iHHvToSiro"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Model Training\n","\n","#### Now, we have prepared models and parameters, and  we can start fine-tuning our model. We load in \"SFTTrainer\" and simply run \"trainer.train()\". During training the loss will be printed every 10 steps according to the logging_steps parameter. Note: I have used my \"Weights&Biases\" API key to train the model"],"metadata":{"id":"U1D9VurVbkJn"}},{"cell_type":"code","source":["from trl import SFTTrainer\n","\n","# Set supervised fine-tuning parameters\n","trainer = SFTTrainer(\n","    model=model,\n","    train_dataset=dataset,\n","    data_collator=collator,\n","    dataset_text_field=\"text\",\n","    tokenizer=tokenizer,\n","    args=training_arguments,\n","    max_seq_length=512,\n","\n","    # Leave this out for regular SFT\n","    peft_config=peft_config,\n",")\n","\n","# Train model\n","trainer.train()\n","\n","# Save QLoRA weights\n","trainer.model.save_pretrained(\"Qwen2-0.5B-qlora\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["f524020dcdbd4a5eb17de1322c93df0f","90d1b6d6b62e4ec59b1bbdf4b9e3097e","721ad143c7b547adac5b79502cd9a98d","276d0315b7ca49e58ea6a007ec41aafd","a8b6ac284b784655b16ff122cfbf9fb7","b373964729454230aae134e1b11c2b3a","0e1ab328b96c4ad08d85a6702acb8005","c10dbc70d29e44b2b18528be97de7e5f","42f9f8e9f22d48159cc652558cedeb34","25789b94222b44088d4aadae38eb70d6","344b114541dd49439ec11b012ab9667b"]},"id":"4KbyaaDVSivP","executionInfo":{"status":"ok","timestamp":1756077609135,"user_tz":420,"elapsed":4687709,"user":{"displayName":"Narahari Marneni","userId":"11245013731051180426"}},"outputId":"3bbaa670-170d-4c75-a3ed-2f5c331d13ba"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length. Will not be supported from version '1.0.0'.\n","\n","Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n","  warnings.warn(message, FutureWarning)\n","/usr/local/lib/python3.12/dist-packages/transformers/training_args.py:1965: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/trl/trainer/sft_trainer.py:307: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/16406 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f524020dcdbd4a5eb17de1322c93df0f"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py:477: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n","  | |_| | '_ \\/ _` / _` |  _/ -_)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","        window._wandbApiKey = new Promise((resolve, reject) => {\n","            function loadScript(url) {\n","            return new Promise(function(resolve, reject) {\n","                let newScript = document.createElement(\"script\");\n","                newScript.onerror = reject;\n","                newScript.onload = resolve;\n","                document.body.appendChild(newScript);\n","                newScript.src = url;\n","            });\n","            }\n","            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n","            const iframe = document.createElement('iframe')\n","            iframe.style.cssText = \"width:0;height:0;border:none\"\n","            document.body.appendChild(iframe)\n","            const handshake = new Postmate({\n","                container: iframe,\n","                url: 'https://wandb.ai/authorize'\n","            });\n","            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n","            handshake.then(function(child) {\n","                child.on('authorize', data => {\n","                    clearTimeout(timeout)\n","                    resolve(data)\n","                });\n","            });\n","            })\n","        });\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n","wandb: Paste an API key from your profile and hit enter:"]},{"name":"stdout","output_type":"stream","text":[" Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n","\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmnarahari\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.21.1"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20250824_220240-5p4kvc7r</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/mnarahari/huggingface/runs/5p4kvc7r' target=\"_blank\">qwen2-sft-chatbot</a></strong> to <a href='https://wandb.ai/mnarahari/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/mnarahari/huggingface' target=\"_blank\">https://wandb.ai/mnarahari/huggingface</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/mnarahari/huggingface/runs/5p4kvc7r' target=\"_blank\">https://wandb.ai/mnarahari/huggingface/runs/5p4kvc7r</a>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n","/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='2050' max='2050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2050/2050 1:17:23, Epoch 0/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>10</td>\n","      <td>2.119700</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>2.026800</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>1.794800</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>1.778100</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>1.753600</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>1.969800</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>2.026500</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>1.833400</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>1.976300</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>1.775400</td>\n","    </tr>\n","    <tr>\n","      <td>110</td>\n","      <td>1.992500</td>\n","    </tr>\n","    <tr>\n","      <td>120</td>\n","      <td>1.997300</td>\n","    </tr>\n","    <tr>\n","      <td>130</td>\n","      <td>1.919100</td>\n","    </tr>\n","    <tr>\n","      <td>140</td>\n","      <td>2.032400</td>\n","    </tr>\n","    <tr>\n","      <td>150</td>\n","      <td>1.849200</td>\n","    </tr>\n","    <tr>\n","      <td>160</td>\n","      <td>1.787200</td>\n","    </tr>\n","    <tr>\n","      <td>170</td>\n","      <td>2.061100</td>\n","    </tr>\n","    <tr>\n","      <td>180</td>\n","      <td>2.011100</td>\n","    </tr>\n","    <tr>\n","      <td>190</td>\n","      <td>1.875900</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>1.881300</td>\n","    </tr>\n","    <tr>\n","      <td>210</td>\n","      <td>1.926400</td>\n","    </tr>\n","    <tr>\n","      <td>220</td>\n","      <td>1.987600</td>\n","    </tr>\n","    <tr>\n","      <td>230</td>\n","      <td>2.135300</td>\n","    </tr>\n","    <tr>\n","      <td>240</td>\n","      <td>1.963800</td>\n","    </tr>\n","    <tr>\n","      <td>250</td>\n","      <td>2.230900</td>\n","    </tr>\n","    <tr>\n","      <td>260</td>\n","      <td>2.127300</td>\n","    </tr>\n","    <tr>\n","      <td>270</td>\n","      <td>1.927700</td>\n","    </tr>\n","    <tr>\n","      <td>280</td>\n","      <td>2.049500</td>\n","    </tr>\n","    <tr>\n","      <td>290</td>\n","      <td>1.949500</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>1.995900</td>\n","    </tr>\n","    <tr>\n","      <td>310</td>\n","      <td>1.980100</td>\n","    </tr>\n","    <tr>\n","      <td>320</td>\n","      <td>2.029300</td>\n","    </tr>\n","    <tr>\n","      <td>330</td>\n","      <td>2.118900</td>\n","    </tr>\n","    <tr>\n","      <td>340</td>\n","      <td>2.059700</td>\n","    </tr>\n","    <tr>\n","      <td>350</td>\n","      <td>1.990100</td>\n","    </tr>\n","    <tr>\n","      <td>360</td>\n","      <td>2.255500</td>\n","    </tr>\n","    <tr>\n","      <td>370</td>\n","      <td>1.920600</td>\n","    </tr>\n","    <tr>\n","      <td>380</td>\n","      <td>1.855100</td>\n","    </tr>\n","    <tr>\n","      <td>390</td>\n","      <td>2.023600</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>2.110100</td>\n","    </tr>\n","    <tr>\n","      <td>410</td>\n","      <td>2.113300</td>\n","    </tr>\n","    <tr>\n","      <td>420</td>\n","      <td>1.914600</td>\n","    </tr>\n","    <tr>\n","      <td>430</td>\n","      <td>2.101100</td>\n","    </tr>\n","    <tr>\n","      <td>440</td>\n","      <td>1.966300</td>\n","    </tr>\n","    <tr>\n","      <td>450</td>\n","      <td>1.985700</td>\n","    </tr>\n","    <tr>\n","      <td>460</td>\n","      <td>1.900900</td>\n","    </tr>\n","    <tr>\n","      <td>470</td>\n","      <td>2.034400</td>\n","    </tr>\n","    <tr>\n","      <td>480</td>\n","      <td>1.846200</td>\n","    </tr>\n","    <tr>\n","      <td>490</td>\n","      <td>2.136300</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>1.917000</td>\n","    </tr>\n","    <tr>\n","      <td>510</td>\n","      <td>1.745400</td>\n","    </tr>\n","    <tr>\n","      <td>520</td>\n","      <td>2.108900</td>\n","    </tr>\n","    <tr>\n","      <td>530</td>\n","      <td>1.966400</td>\n","    </tr>\n","    <tr>\n","      <td>540</td>\n","      <td>2.044700</td>\n","    </tr>\n","    <tr>\n","      <td>550</td>\n","      <td>2.118600</td>\n","    </tr>\n","    <tr>\n","      <td>560</td>\n","      <td>2.078600</td>\n","    </tr>\n","    <tr>\n","      <td>570</td>\n","      <td>1.982400</td>\n","    </tr>\n","    <tr>\n","      <td>580</td>\n","      <td>1.738500</td>\n","    </tr>\n","    <tr>\n","      <td>590</td>\n","      <td>2.072300</td>\n","    </tr>\n","    <tr>\n","      <td>600</td>\n","      <td>1.852200</td>\n","    </tr>\n","    <tr>\n","      <td>610</td>\n","      <td>2.116700</td>\n","    </tr>\n","    <tr>\n","      <td>620</td>\n","      <td>2.014900</td>\n","    </tr>\n","    <tr>\n","      <td>630</td>\n","      <td>2.166300</td>\n","    </tr>\n","    <tr>\n","      <td>640</td>\n","      <td>1.940400</td>\n","    </tr>\n","    <tr>\n","      <td>650</td>\n","      <td>1.778200</td>\n","    </tr>\n","    <tr>\n","      <td>660</td>\n","      <td>2.043700</td>\n","    </tr>\n","    <tr>\n","      <td>670</td>\n","      <td>1.992600</td>\n","    </tr>\n","    <tr>\n","      <td>680</td>\n","      <td>1.942000</td>\n","    </tr>\n","    <tr>\n","      <td>690</td>\n","      <td>2.021300</td>\n","    </tr>\n","    <tr>\n","      <td>700</td>\n","      <td>1.859100</td>\n","    </tr>\n","    <tr>\n","      <td>710</td>\n","      <td>1.767400</td>\n","    </tr>\n","    <tr>\n","      <td>720</td>\n","      <td>1.812000</td>\n","    </tr>\n","    <tr>\n","      <td>730</td>\n","      <td>2.018000</td>\n","    </tr>\n","    <tr>\n","      <td>740</td>\n","      <td>1.735400</td>\n","    </tr>\n","    <tr>\n","      <td>750</td>\n","      <td>1.951300</td>\n","    </tr>\n","    <tr>\n","      <td>760</td>\n","      <td>1.745800</td>\n","    </tr>\n","    <tr>\n","      <td>770</td>\n","      <td>2.013900</td>\n","    </tr>\n","    <tr>\n","      <td>780</td>\n","      <td>1.847700</td>\n","    </tr>\n","    <tr>\n","      <td>790</td>\n","      <td>1.880700</td>\n","    </tr>\n","    <tr>\n","      <td>800</td>\n","      <td>2.029800</td>\n","    </tr>\n","    <tr>\n","      <td>810</td>\n","      <td>1.943100</td>\n","    </tr>\n","    <tr>\n","      <td>820</td>\n","      <td>1.895100</td>\n","    </tr>\n","    <tr>\n","      <td>830</td>\n","      <td>1.739900</td>\n","    </tr>\n","    <tr>\n","      <td>840</td>\n","      <td>2.007700</td>\n","    </tr>\n","    <tr>\n","      <td>850</td>\n","      <td>1.756600</td>\n","    </tr>\n","    <tr>\n","      <td>860</td>\n","      <td>1.932300</td>\n","    </tr>\n","    <tr>\n","      <td>870</td>\n","      <td>1.731300</td>\n","    </tr>\n","    <tr>\n","      <td>880</td>\n","      <td>1.858400</td>\n","    </tr>\n","    <tr>\n","      <td>890</td>\n","      <td>1.949100</td>\n","    </tr>\n","    <tr>\n","      <td>900</td>\n","      <td>1.888900</td>\n","    </tr>\n","    <tr>\n","      <td>910</td>\n","      <td>1.828700</td>\n","    </tr>\n","    <tr>\n","      <td>920</td>\n","      <td>1.971200</td>\n","    </tr>\n","    <tr>\n","      <td>930</td>\n","      <td>1.950700</td>\n","    </tr>\n","    <tr>\n","      <td>940</td>\n","      <td>1.626200</td>\n","    </tr>\n","    <tr>\n","      <td>950</td>\n","      <td>1.782100</td>\n","    </tr>\n","    <tr>\n","      <td>960</td>\n","      <td>1.925800</td>\n","    </tr>\n","    <tr>\n","      <td>970</td>\n","      <td>1.791700</td>\n","    </tr>\n","    <tr>\n","      <td>980</td>\n","      <td>1.568200</td>\n","    </tr>\n","    <tr>\n","      <td>990</td>\n","      <td>1.752000</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>1.818700</td>\n","    </tr>\n","    <tr>\n","      <td>1010</td>\n","      <td>1.694900</td>\n","    </tr>\n","    <tr>\n","      <td>1020</td>\n","      <td>1.775200</td>\n","    </tr>\n","    <tr>\n","      <td>1030</td>\n","      <td>1.738400</td>\n","    </tr>\n","    <tr>\n","      <td>1040</td>\n","      <td>1.591900</td>\n","    </tr>\n","    <tr>\n","      <td>1050</td>\n","      <td>1.778200</td>\n","    </tr>\n","    <tr>\n","      <td>1060</td>\n","      <td>1.613000</td>\n","    </tr>\n","    <tr>\n","      <td>1070</td>\n","      <td>1.911100</td>\n","    </tr>\n","    <tr>\n","      <td>1080</td>\n","      <td>1.515000</td>\n","    </tr>\n","    <tr>\n","      <td>1090</td>\n","      <td>1.603300</td>\n","    </tr>\n","    <tr>\n","      <td>1100</td>\n","      <td>1.741900</td>\n","    </tr>\n","    <tr>\n","      <td>1110</td>\n","      <td>1.754300</td>\n","    </tr>\n","    <tr>\n","      <td>1120</td>\n","      <td>1.741100</td>\n","    </tr>\n","    <tr>\n","      <td>1130</td>\n","      <td>1.824800</td>\n","    </tr>\n","    <tr>\n","      <td>1140</td>\n","      <td>1.728200</td>\n","    </tr>\n","    <tr>\n","      <td>1150</td>\n","      <td>1.755300</td>\n","    </tr>\n","    <tr>\n","      <td>1160</td>\n","      <td>1.642200</td>\n","    </tr>\n","    <tr>\n","      <td>1170</td>\n","      <td>1.666400</td>\n","    </tr>\n","    <tr>\n","      <td>1180</td>\n","      <td>1.447200</td>\n","    </tr>\n","    <tr>\n","      <td>1190</td>\n","      <td>1.798000</td>\n","    </tr>\n","    <tr>\n","      <td>1200</td>\n","      <td>1.834100</td>\n","    </tr>\n","    <tr>\n","      <td>1210</td>\n","      <td>1.520700</td>\n","    </tr>\n","    <tr>\n","      <td>1220</td>\n","      <td>1.616800</td>\n","    </tr>\n","    <tr>\n","      <td>1230</td>\n","      <td>1.604700</td>\n","    </tr>\n","    <tr>\n","      <td>1240</td>\n","      <td>1.598900</td>\n","    </tr>\n","    <tr>\n","      <td>1250</td>\n","      <td>1.508500</td>\n","    </tr>\n","    <tr>\n","      <td>1260</td>\n","      <td>1.858000</td>\n","    </tr>\n","    <tr>\n","      <td>1270</td>\n","      <td>1.605000</td>\n","    </tr>\n","    <tr>\n","      <td>1280</td>\n","      <td>1.776400</td>\n","    </tr>\n","    <tr>\n","      <td>1290</td>\n","      <td>1.860800</td>\n","    </tr>\n","    <tr>\n","      <td>1300</td>\n","      <td>1.769400</td>\n","    </tr>\n","    <tr>\n","      <td>1310</td>\n","      <td>1.691300</td>\n","    </tr>\n","    <tr>\n","      <td>1320</td>\n","      <td>1.522800</td>\n","    </tr>\n","    <tr>\n","      <td>1330</td>\n","      <td>1.586500</td>\n","    </tr>\n","    <tr>\n","      <td>1340</td>\n","      <td>1.495900</td>\n","    </tr>\n","    <tr>\n","      <td>1350</td>\n","      <td>1.697300</td>\n","    </tr>\n","    <tr>\n","      <td>1360</td>\n","      <td>1.510300</td>\n","    </tr>\n","    <tr>\n","      <td>1370</td>\n","      <td>1.528300</td>\n","    </tr>\n","    <tr>\n","      <td>1380</td>\n","      <td>1.654800</td>\n","    </tr>\n","    <tr>\n","      <td>1390</td>\n","      <td>1.529000</td>\n","    </tr>\n","    <tr>\n","      <td>1400</td>\n","      <td>1.368800</td>\n","    </tr>\n","    <tr>\n","      <td>1410</td>\n","      <td>1.589200</td>\n","    </tr>\n","    <tr>\n","      <td>1420</td>\n","      <td>1.693100</td>\n","    </tr>\n","    <tr>\n","      <td>1430</td>\n","      <td>1.511600</td>\n","    </tr>\n","    <tr>\n","      <td>1440</td>\n","      <td>1.500900</td>\n","    </tr>\n","    <tr>\n","      <td>1450</td>\n","      <td>1.437900</td>\n","    </tr>\n","    <tr>\n","      <td>1460</td>\n","      <td>1.738700</td>\n","    </tr>\n","    <tr>\n","      <td>1470</td>\n","      <td>1.398900</td>\n","    </tr>\n","    <tr>\n","      <td>1480</td>\n","      <td>1.492000</td>\n","    </tr>\n","    <tr>\n","      <td>1490</td>\n","      <td>1.496200</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>1.651700</td>\n","    </tr>\n","    <tr>\n","      <td>1510</td>\n","      <td>1.319600</td>\n","    </tr>\n","    <tr>\n","      <td>1520</td>\n","      <td>1.279500</td>\n","    </tr>\n","    <tr>\n","      <td>1530</td>\n","      <td>1.616400</td>\n","    </tr>\n","    <tr>\n","      <td>1540</td>\n","      <td>1.416100</td>\n","    </tr>\n","    <tr>\n","      <td>1550</td>\n","      <td>1.294300</td>\n","    </tr>\n","    <tr>\n","      <td>1560</td>\n","      <td>1.454400</td>\n","    </tr>\n","    <tr>\n","      <td>1570</td>\n","      <td>1.447100</td>\n","    </tr>\n","    <tr>\n","      <td>1580</td>\n","      <td>1.456400</td>\n","    </tr>\n","    <tr>\n","      <td>1590</td>\n","      <td>1.437800</td>\n","    </tr>\n","    <tr>\n","      <td>1600</td>\n","      <td>1.552700</td>\n","    </tr>\n","    <tr>\n","      <td>1610</td>\n","      <td>1.388100</td>\n","    </tr>\n","    <tr>\n","      <td>1620</td>\n","      <td>1.401300</td>\n","    </tr>\n","    <tr>\n","      <td>1630</td>\n","      <td>1.565100</td>\n","    </tr>\n","    <tr>\n","      <td>1640</td>\n","      <td>1.571000</td>\n","    </tr>\n","    <tr>\n","      <td>1650</td>\n","      <td>1.338800</td>\n","    </tr>\n","    <tr>\n","      <td>1660</td>\n","      <td>1.575500</td>\n","    </tr>\n","    <tr>\n","      <td>1670</td>\n","      <td>1.434900</td>\n","    </tr>\n","    <tr>\n","      <td>1680</td>\n","      <td>1.628000</td>\n","    </tr>\n","    <tr>\n","      <td>1690</td>\n","      <td>1.469200</td>\n","    </tr>\n","    <tr>\n","      <td>1700</td>\n","      <td>1.578700</td>\n","    </tr>\n","    <tr>\n","      <td>1710</td>\n","      <td>1.571800</td>\n","    </tr>\n","    <tr>\n","      <td>1720</td>\n","      <td>1.498700</td>\n","    </tr>\n","    <tr>\n","      <td>1730</td>\n","      <td>1.457600</td>\n","    </tr>\n","    <tr>\n","      <td>1740</td>\n","      <td>1.549500</td>\n","    </tr>\n","    <tr>\n","      <td>1750</td>\n","      <td>1.451000</td>\n","    </tr>\n","    <tr>\n","      <td>1760</td>\n","      <td>1.514200</td>\n","    </tr>\n","    <tr>\n","      <td>1770</td>\n","      <td>1.550000</td>\n","    </tr>\n","    <tr>\n","      <td>1780</td>\n","      <td>1.567300</td>\n","    </tr>\n","    <tr>\n","      <td>1790</td>\n","      <td>1.391700</td>\n","    </tr>\n","    <tr>\n","      <td>1800</td>\n","      <td>1.567000</td>\n","    </tr>\n","    <tr>\n","      <td>1810</td>\n","      <td>1.470000</td>\n","    </tr>\n","    <tr>\n","      <td>1820</td>\n","      <td>1.397100</td>\n","    </tr>\n","    <tr>\n","      <td>1830</td>\n","      <td>1.276900</td>\n","    </tr>\n","    <tr>\n","      <td>1840</td>\n","      <td>1.564500</td>\n","    </tr>\n","    <tr>\n","      <td>1850</td>\n","      <td>1.439700</td>\n","    </tr>\n","    <tr>\n","      <td>1860</td>\n","      <td>1.242300</td>\n","    </tr>\n","    <tr>\n","      <td>1870</td>\n","      <td>1.415800</td>\n","    </tr>\n","    <tr>\n","      <td>1880</td>\n","      <td>1.541300</td>\n","    </tr>\n","    <tr>\n","      <td>1890</td>\n","      <td>1.402600</td>\n","    </tr>\n","    <tr>\n","      <td>1900</td>\n","      <td>1.548300</td>\n","    </tr>\n","    <tr>\n","      <td>1910</td>\n","      <td>1.267400</td>\n","    </tr>\n","    <tr>\n","      <td>1920</td>\n","      <td>1.453600</td>\n","    </tr>\n","    <tr>\n","      <td>1930</td>\n","      <td>1.497400</td>\n","    </tr>\n","    <tr>\n","      <td>1940</td>\n","      <td>1.347300</td>\n","    </tr>\n","    <tr>\n","      <td>1950</td>\n","      <td>1.244600</td>\n","    </tr>\n","    <tr>\n","      <td>1960</td>\n","      <td>1.433700</td>\n","    </tr>\n","    <tr>\n","      <td>1970</td>\n","      <td>1.261500</td>\n","    </tr>\n","    <tr>\n","      <td>1980</td>\n","      <td>1.514100</td>\n","    </tr>\n","    <tr>\n","      <td>1990</td>\n","      <td>1.218000</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>1.577400</td>\n","    </tr>\n","    <tr>\n","      <td>2010</td>\n","      <td>1.471600</td>\n","    </tr>\n","    <tr>\n","      <td>2020</td>\n","      <td>1.466800</td>\n","    </tr>\n","    <tr>\n","      <td>2030</td>\n","      <td>1.360100</td>\n","    </tr>\n","    <tr>\n","      <td>2040</td>\n","      <td>1.618200</td>\n","    </tr>\n","    <tr>\n","      <td>2050</td>\n","      <td>1.348900</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n"]}]},{"cell_type":"markdown","source":["# Merge Adapter\n","\n","#### After trained QLoRA weights, then combine them with the original weights to use them. We reload the model in 16 bits, instead of the quantized 4 bits, to merge the weights. Although the tokenizer was not updated during training, we save it to the same folder as the model for easier access."],"metadata":{"id":"LqS-qsmkqgEH"}},{"cell_type":"code","source":["from peft import AutoPeftModelForCausalLM\n","\n","model = AutoPeftModelForCausalLM.from_pretrained(\n","    \"Qwen2-0.5B-qlora\",\n","    low_cpu_mem_usage=True,\n","    device_map=\"auto\",\n",")\n","\n","# Merge LoRA and base model\n","merged_model = model.merge_and_unload()"],"metadata":{"id":"N4EI5WobvFId"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Inference\n","#### After merging the adapter with the base model, we can use it with the prompt template that is defined earlier."],"metadata":{"id":"B7bgdZ-JvVBO"}},{"cell_type":"code","source":["from transformers import pipeline\n","\n","print (\"Test Example-1:\")\n","# Use the predefined prompt template\n","question = \"\"\"User:\n","What is Blood Pressure?\n","Assistant:\n","\"\"\"\n","\n","# Run our instruction-tuned model\n","pipe = pipeline(task=\"text-generation\", model=merged_model, tokenizer=tokenizer)\n","print(pipe(question)[0][\"generated_text\"])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HkwMnrmCvVzz","executionInfo":{"status":"ok","timestamp":1756078581735,"user_tz":420,"elapsed":496,"user":{"displayName":"Narahari Marneni","userId":"11245013731051180426"}},"outputId":"f5596acc-6f24-4895-c6e7-28763e116299"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Test Example-1:\n","User:\n","What is Blood Pressure?\n","Assistant:\n","Blood pressure is the force of blood pushing against the walls\n"]}]},{"cell_type":"code","source":["print (\"Test Example-2:\")\n","# Use the predefined prompt template\n","question = \"\"\"User:\n","What is Diabetes?\n","Assistant:\n","\"\"\"\n","\n","# Run our instruction-tuned model\n","pipe = pipeline(task=\"text-generation\", model=merged_model, tokenizer=tokenizer)\n","print(pipe(question)[0][\"generated_text\"])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iU9lc1dYb4SD","executionInfo":{"status":"ok","timestamp":1756078587865,"user_tz":420,"elapsed":356,"user":{"displayName":"Narahari Marneni","userId":"11245013731051180426"}},"outputId":"9d89fd3a-378a-4733-db9a-36a6be64f3a9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Test Example-2:\n","User:\n","What is Diabetes?\n","Assistant:\n","People with type 2 diabetes are more likely to develop type\n"]}]},{"cell_type":"code","source":["print (\"Test Example-3:\")\n","# Use the predefined prompt template\n","question = \"\"\"User:\n","What causes Osteoarthritis?\n","Assistant:\n","\"\"\"\n","\n","# Run our instruction-tuned model\n","pipe = pipeline(task=\"text-generation\", model=merged_model, tokenizer=tokenizer)\n","print(pipe(question)[0][\"generated_text\"])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ssmodtPGb4Yu","executionInfo":{"status":"ok","timestamp":1756078650417,"user_tz":420,"elapsed":685,"user":{"displayName":"Narahari Marneni","userId":"11245013731051180426"}},"outputId":"4e8a977c-ec26-4c80-c97b-594a67e2d59d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Test Example-3:\n","User:\n","What causes Osteoarthritis?\n","Assistant:\n","People who have osteoarthritis often\n"]}]},{"cell_type":"code","source":["print (\"Test Example-4:\")\n","# Use the predefined prompt template\n","question = \"\"\"User:\n","What is (are) Anxiety Disorders?\n","Assistant:\n","\"\"\"\n","\n","# Run our instruction-tuned model\n","pipe = pipeline(task=\"text-generation\", model=merged_model, tokenizer=tokenizer)\n","print(pipe(question)[0][\"generated_text\"])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cffx_LOUb4hL","executionInfo":{"status":"ok","timestamp":1756078748687,"user_tz":420,"elapsed":438,"user":{"displayName":"Narahari Marneni","userId":"11245013731051180426"}},"outputId":"62e0b8f7-cca6-43a6-8a69-fec27aaa9c38"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Test Example-4:\n","User:\n","What is (are) Anxiety Disorders?\n","Assistant:\n"," Anxiety disorders are a group of psychological problems\n"]}]},{"cell_type":"markdown","source":["# Evaluating Generative Models\n","\n","#### Evaluating generative models poses a significant challenge. Generative models are used across many diverse use cases, making it a challenge to rely on a singular metric for judgment. Given their probabilistic nature, generative models do not necessarily generate consistent outputs. No one metric is perfect for all use cases.\n","\n","#### One common metrics category for comparing generative models is wordlevel evaluation. These classic techniques compare a reference dataset with the generated tokens on a token(set) level. Common word-level metrics include perplexity, ROUGE, BLEU, and BERTScore.\n","\n","#### A common method for evaluating generative models on language generation and understanding tasks is on well-known and public benchmarks, such as MMLU,GLUE, TruthfulQA, GSM8k, and HellaSwag. These benchmarks give us information about basic language understanding but also complex analytical answering.\n","\n","#### Aside from natural language tasks, some models specialize in other domains, like programming. These models tend to be evaluated on different benchmarks, such as HumanEval, which consists of challenging programming tasks for the model to solve."],"metadata":{"id":"km1QcC9AOOko"}},{"cell_type":"markdown","source":["# Preference-Tuning / Alignment / Reinforcement Learning from Human Feedback (RLHF)\n","\n","#### Although the present model can now follow instructions, we can further improve its performance by implementing additional fine-tuning technics during the training phase.\n","\n","#### A common method to fine-tune the LLM with the trained reward model is Proximal Policy Optimization (PPO). PPO is a popular reinforcement technique that optimizes the instruction-tuned LLM by making sure that the LLM does not deviate too much from the expected rewards.\n","\n","#### A disadvantage of PPO is that it is a complex method that needs to train at least two models, the reward model and the LLM, which can be more costly than perhaps necessary.\n","\n","#### Direct Preference Optimization (DPO) is an alternative to PPO and does away with the reinforcement-based learning procedure. Instead of using the reward model to judge the quality of a generation, we let the LLM itself do that. Compared to PPO, the DPO method is found to be more stable during training and more accurate.\n","\n","#### The combination of SFT+DPO is a great way to first fine-tune the model to perform basic chatting and then align its answers with human preference. However, it is computationally expensive since we need to perform two training loops and potentially tweak the parameters in two processes.\n","\n","#### Recently, a new method, called Odds Ratio Preference Optimization (ORPO) of aligning preferences has developed by J Hong (2024). This method combines SFT and DPO into a single training process. It removes the need to perform two separate training loops, further simplifying the training process while allowing for the use of QLoRA."],"metadata":{"id":"mMMdVek3fUT5"}},{"cell_type":"markdown","source":["# Conclusion\n","\n","#### In project, I have developed a Chatbot (generative AI model) by fine-tuning pretrained LLM. I have used a lite weight pretrained LLM model \"\"Qwen/Qwen2-0.5B-Instruct\" and fine-tuned on the given data set \"mle_screening_dataset.csv\". The fine-tuning was performed by making use of parameter-efficient fine-tuning (PEFT) through the low-rank adaptation (LoRA) technique and the LoRA was extended through quantization, a technique for reducing memory constraints when representing the parameters of the model and adapters. The model performance was tested with four different examples. Since I have used a lite weight pretarined model, there is a limitation on the number of characters in text generation.\n","\n","#### We can further improve the model by utilizing large medical-specialized LLMs such as BioGPT, MedAlpaca, or PMC-LLaMA. For deployment in healthcare (HIPAA-compliant), even we can use better models like Azure OpenAI GPT-4 or Google Med-PaLM 2. Additionally, we can run a RAG pipeline with trusted medical sources to get most accurate answers for the questions asked.\n"],"metadata":{"id":"o1e8hJj9npjq"}}]}
